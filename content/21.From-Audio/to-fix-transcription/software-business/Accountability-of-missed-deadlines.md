**Accountability of missed deadlines.**

In development, especially on bigger projects, there tends to be these massive deadlines which cannot be missed until of course they are missed and have to be followed and account that much into delivering it. And usually cause a huge amounts of counter cases for being cut, and a huge amount of compromises just to meet the deadline.

And of course most of them are totally made up, they totally have no sense on reality, they are just some date in the future that could be two months, could be three months from now, or a year from now.

And in a way they tend to have a really bad side effect of making teams and people make bad decisions. And that is a problem because a lot of security vulnerabilities get created or germinated in those deadlines. 

So, I think it is very important to understand for example what is being done, more importantly understand the side effects and the pollution created by for example trying certain things. Because what you actually then have is a mapping of what needs to be fixed and what needs to happen next in order to address that deadline.

The other thing that is important is to learn from the past, so if you have a company that has a massive culture of understanding and delivering things in a very tight schedule but in way that it doesn't compromise quality and it doesn't really create problems or side effects then that is okay.

But in most companies what happens is they struggle to deliver, and when they deliver there is a huge amount of change that needs to occur and the huge amount of [inaudible 00:02:14] that occurs which basically means that you don't have a culture.

So what you need to do is learn from the past and understand the capabilities of the application of the company. The capabilities of the organization of delivering real solutions in the real world and then learn from that in a way experience from the past to understand their capabilities.

So the consequence of this might be that the company says or the team says that we can predict, we can model, we don't know when to deliver it. And that is better because sometimes it is better to do incremental changes into a bigger change than to actually try to change too many things at the same time where what you will end up having is you will end up having the worst of both worlds.

So, a good example is if team A is developing a module and team B is also developing a big part of another component that is also going to change. What happens when team A starts to need a production quality team B component. If team B isn't there or is just hasn't delivered or isn't fully functional, then you end up having the worst of both worlds because team A cannot develop their own solution, they have to use team B's product. But team B's product isn't good enough or isn't resilient enough or doesn't deliver where it is supposed to deliver so you get this issue.

I like sometimes again to go back to [inaudible 00:03:53] model, if you have dependencies, then first deal with the dependency and make it solid, make it robust. In fact I think the best model is never to reinvent the wheel in vacuum because it means the tool hasn't hit the real world, is to take something that works, has some issues, and you basically develop the first version, 100% compatible with what is there in a new technology stack and literally you build it as a proxy to the existing solution. So that you can actually replace it in and out, make sure they work, make sure it survives the real world, and then you start building the other modules.

And what happens is that module that is now in the real world should eventually start to replace junks of the previous system until you eventually don't have a lot of stuff left.

And that is basically a much better model where you take [inaudible 00:04:50]